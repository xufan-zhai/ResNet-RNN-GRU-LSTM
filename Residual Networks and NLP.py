# -*- coding: utf-8 -*-
"""Google_colab_usage2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U-qCRT0cusmrLfu4kOxYD10VRS0ddlyG
"""

# The official site of how to use google colab: https://pytorch.org/tutorials/beginner/colab.html

# The code starting from here help us to connect our code in colab to our google drive
# i.e. we use can read and save file from a driectory in our google drive as we are using our own computer
# Therefore, please run this part of code first before you start running other code
# When you run this part of the code, it will require you to authorize some rights to it, just follow the instruction is OK

# import the corresponding library
from google.colab import drive
drive.mount('/content/gdrive')

# Install a Drive FUSE wrapper.
# https://github.com/astrada/google-drive-ocamlfuse
!apt-get install -y -qq software-properties-common python-software-properties module-init-tools
!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null
!apt-get update -qq 2>&1 > /dev/null
!apt-get -y install -qq google-drive-ocamlfuse fuse

# Generate auth tokens for Colab
from google.colab import auth
auth.authenticate_user()

# Generate creds for the Drive FUSE library.
from oauth2client.client import GoogleCredentials
creds = GoogleCredentials.get_application_default()
import getpass
!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL
vcode = getpass.getpass()
!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}

# Create a directory and mount Google Drive using that directory.
!mkdir -p drive
!google-drive-ocamlfuse drive

# After running this part of code, we have successfully connected our drive to our google colab
# And we are at the position that is one layer higher than the root of our google drive
# The next thing we need to do is go to our work directory
# Suppose we make a directory called "try_colab" under the root of our google drive and we want to work at this directory (saving and loading file from it)

import os
# suppose you make a directory called "try_colab" under your google drive
# you can change this path to the directory you want to work at 
path = "./drive/m231/hw2/PyTorch_CIFAR10-master/"
os.chdir(path)
os.listdir('./')

# This is an example of load the pretrained model
# You should run this after you have installed the required packages and download the weights


from cifar10_models import resnet



# Pretrained model
res18 = resnet.resnet18(pretrained=True)
res34 = resnet.resnet34(pretrained=True)
res50 = resnet.resnet50(pretrained=True)

res18.eval()
res34.eval()
res50.eval()

import os#rnn/gru/lstm
!mkdir -p drive
!google-drive-ocamlfuse drive
path = "./drive/m231/hw2/"
os.chdir(path)
os.listdir('./')

import torch
import torchvision
import torchvision.transforms as transforms
transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2471, 0.2435, 0.2616))])

batch_size = 32

trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,
                                          shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                       download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,
                                         shuffle=False, num_workers=2)

classes = ('plane', 'car', 'bird', 'cat',
           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

#test for res18

correct_test = 0
total_test = 0

with torch.no_grad():
      for data in testloader:
          images, labels = data
          output = res18(images)
          _, predicted = torch.max(output.data, 1)
          total_test += labels.size(0)
          correct_test += (predicted == labels).sum().item()
      test_accuracy18=correct_test/total_test
print(test_accuracy18)

#test for resnet34

correct_test = 0
total_test = 0

with torch.no_grad():
      for data in testloader:
          images, labels = data
          output = res34(images)
          _, predicted = torch.max(output.data, 1)
          total_test += labels.size(0)
          correct_test += (predicted == labels).sum().item()
      test_accuracy34=correct_test/total_test
print(test_accuracy34)

#test for resnet50

correct_test = 0
total_test = 0

with torch.no_grad():
      for data in testloader:
          images, labels = data
          output = res50(images)
          _, predicted = torch.max(output.data, 1)
          total_test += labels.size(0)
          correct_test += (predicted == labels).sum().item()

      test_accuracy=correct_test/total_test
print(test_accuracy)

res50.eval()

import matplotlib.pyplot as plt
plt.bar(['Resnet18','Resnet34','Resnet50'],[test_accuracy18,test_accuracy34,test_accuracy])

import os#rnn/gru/lstm
!mkdir -p drive
!google-drive-ocamlfuse drive
path = "./drive/m231/hw2/"
os.chdir(path)
os.listdir('./')


import time

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader

from tqdm import tqdm_notebook
from sklearn.preprocessing import MinMaxScaler

# Define data root directory
data_dir = "./GRU_Prediction-master/data/"
pd.read_csv(data_dir + 'AEP_hourly.csv').head()

# The scaler objects will be stored in this dictionary so that our output test data from the model can be re-scaled during evaluation
label_scalers = {}

train_x = []
test_x = {}
test_y = {}

for file in tqdm_notebook(os.listdir(data_dir)): 
    # Skipping the files we're not using
    if file[-4:] != ".csv" or file == "pjm_hourly_est.csv":
        continue
    
    # Store csv file in a Pandas DataFrame
    df = pd.read_csv('{}/{}'.format(data_dir, file), parse_dates=[0])
    # Processing the time data into suitable input formats
    df['hour'] = df.apply(lambda x: x['Datetime'].hour,axis=1)
    df['dayofweek'] = df.apply(lambda x: x['Datetime'].dayofweek,axis=1)
    df['month'] = df.apply(lambda x: x['Datetime'].month,axis=1)
    df['dayofyear'] = df.apply(lambda x: x['Datetime'].dayofyear,axis=1)
    df = df.sort_values("Datetime").drop("Datetime",axis=1)
    
    # Scaling the input data
    sc = MinMaxScaler()
    label_sc = MinMaxScaler()
    data = sc.fit_transform(df.values)
    # Obtaining the Scale for the labels(usage data) so that output can be re-scaled to actual value during evaluation
    label_sc.fit(df.iloc[:,0].values.reshape(-1,1))
    label_scalers[file] = label_sc
    
    # Define lookback period and split inputs/labels
    lookback = 90
    inputs = np.zeros((len(data)-lookback,lookback,df.shape[1]))
    labels = np.zeros(len(data)-lookback)
    
    for i in range(lookback, len(data)):
        inputs[i-lookback] = data[i-lookback:i]
        labels[i-lookback] = data[i,0]
    inputs = inputs.reshape(-1,lookback,df.shape[1])
    labels = labels.reshape(-1,1)
    
    # Split data into train/test portions and combining all data from different files into a single array
    test_portion = int(0.1*len(inputs))
    if len(train_x) == 0:
        train_x = inputs[:-test_portion]
        train_y = labels[:-test_portion]
    else:
        train_x = np.concatenate((train_x,inputs[:-test_portion]))
        train_y = np.concatenate((train_y,labels[:-test_portion]))
    test_x[file] = (inputs[-test_portion:])
    test_y[file] = (labels[-test_portion:])

batch_size = 1024
train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))
train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)

# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False
is_cuda = torch.cuda.is_available()

# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.
if is_cuda:
    device = torch.device("cuda")
else:
    device = torch.device("cpu")

class GRUNet(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):
        super(GRUNet, self).__init__()
        self.hidden_dim = hidden_dim
        self.n_layers = n_layers
        
        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)
        self.fc = nn.Linear(hidden_dim, output_dim)
        self.relu = nn.ReLU()
        
    def forward(self, x, h):
        out, h = self.gru(x, h)
        out = self.fc(self.relu(out[:,-1]))
        return out, h
    
    def init_hidden(self, batch_size):
        weight = next(self.parameters()).data
        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)
        return hidden
class RNN(nn.Module):
    def __init__(self, input_size, output_size, hidden_dim, n_layers):
        super(RNN, self).__init__()

        # Defining some parameters
        self.hidden_dim = hidden_dim
        self.n_layers = n_layers

        #Defining the layers
        # RNN Layer
        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   
        # Fully connected layer
        self.fc = nn.Linear(hidden_dim, output_size)

      
    
    def forward(self, x):
        
        batch_size = x.size(0)

        # Initializing hidden state for first input using method defined below
        hidden = self.init_hidden(batch_size)

        # Passing in the input and hidden state into the model and obtaining outputs
        out, hidden = self.rnn(x, hidden)
        
        # Reshaping the outputs such that it can be fit into the fully connected layer
        out = out.contiguous().view(-1,90,self.hidden_dim)
        out = self.fc(out)
        out=out[:,-1,:]
      
   
        return out, hidden
    
    def init_hidden(self, batch_size):
        # This method generates the first hidden state of zeros which we'll use in the forward pass
        # We'll send the tensor holding the hidden state to the device we specified earlier as well
        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)
        return hidden
class LSTMNet(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):
        super(LSTMNet, self).__init__()
        self.hidden_dim = hidden_dim
        self.n_layers = n_layers
        
        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)
        self.fc = nn.Linear(hidden_dim, output_dim)
        self.relu = nn.ReLU()
        
    def forward(self, x, h):
        out, h = self.lstm(x, h)
        out = self.fc(self.relu(out[:,-1]))
        return out, h
    
    def init_hidden(self, batch_size):
        weight = next(self.parameters()).data
        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),
                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))
        return hidden

lr=0.0001
RNN_model = train(train_loader, lr, model_type="RNN")

loss_rnn2=np.array([0.00078466 ,0.00035292])
loss_rnn3=np.array([0.00034738 ,0.00019128])

import numpy as np
def train(train_loader, learn_rate, hidden_dim=256, EPOCHS=2, model_type="GRU"):

    
    # Setting common hyperparameters
    input_dim = next(iter(train_loader))[0].shape[2]
    #print(input_dim)
    output_dim = 1
    n_layers = 3
    # Instantiating the models
    if model_type == "GRU":
        model = GRUNet(input_dim, hidden_dim, output_dim, n_layers)
    if model_type=="LSTM":
        model = LSTMNet(input_dim, hidden_dim, output_dim, n_layers)
    if model_type=="RNN":
        #print(input_dim,output_dim )
        model = RNN(input_dim,output_dim , hidden_dim, n_layers)
    model.to(device)
    
    # Defining loss function and optimizer
    criterion = nn.MSELoss()
      #criterion=nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)
    
    print("Starting Training of {} model".format(model_type))
    epoch_times = []
    # Start training loop
    testloss_vec=np.empty(2)
    for epoch in range(1,EPOCHS+1):
        model.train()
        
        start_time = time.clock()
        h = model.init_hidden(batch_size)
        avg_loss = 0.
        counter = 0
        for x, label in train_loader:
            #print(label.size())
            counter += 1
            if model_type == "GRU":
                h = h.data
            if model_type == "LSTM":
                h = tuple([e.data for e in h])
            if model_type=="RNN":
                h= h.to(device)
            model.zero_grad()
            
            if model_type!= "RNN":
              out, h = model(x.to(device).float(), h)
            else:
              out,h= model(x.to(device).float())
              #print(out.size())
              #print(label.size())
              #print(out.size())
            #loss = criterion(out, label.to(device).long())
            loss = criterion(out, label.to(device).float())
            loss.backward()
            optimizer.step()
            avg_loss += loss.item()
            if counter%200 == 0:
                print("Epoch {}......Step: {}/{}....... Average Loss for Epoch: {}".format(epoch, counter, len(train_loader), avg_loss/counter))
        current_time = time.clock()
        model.eval()
        test_lo=0
        with torch.no_grad():
            for i in test_x.keys():
              inp = torch.from_numpy(np.array(test_x[i]))
              labs = torch.from_numpy(np.array(test_y[i]))
              h = model.init_hidden(inp.shape[0])
              if model_type!="RNN":
                out, h = model(inp.to(device).float(), h)
              else:
                out, h = model(inp.to(device).float())      
              loss = criterion(out.to(device), labs.to(device))
              test_lo += loss.item()
        testloss_vec[epoch-1]=test_lo
        print("Epoch {}/{} Done, Total Loss: {}".format(epoch, EPOCHS, avg_loss/len(train_loader)))
        print("Total Time Elapsed: {} seconds".format(str(current_time-start_time)))
        epoch_times.append(current_time-start_time)
    print("test loss:")
    print(testloss_vec/len(test_y))
    print("Total Training Time: {} seconds".format(str(sum(epoch_times))))
    return model

def evaluate(model, test_x, test_y, label_scalers,model_type="LSTM"):
    model.eval()
    outputs = []
    targets = []
    start_time = time.clock()
    criterion=nn.MSELoss()
    for i in test_x.keys():
        inp = torch.from_numpy(np.array(test_x[i]))
        labs = torch.from_numpy(np.array(test_y[i]))
        h = model.init_hidden(inp.shape[0])
        if model_type!="RNN":
          out, h = model(inp.to(device).float(), h)
        else:
          out, h = model(inp.to(device).float())
        outputs.append(label_scalers[i].inverse_transform(out.cpu().detach().numpy()).reshape(-1))
        targets.append(label_scalers[i].inverse_transform(labs.numpy()).reshape(-1))
    print("Evaluation Time: {}".format(str(time.clock()-start_time)))
    sMAPE = 0
    for i in range(len(outputs)):
        sMAPE += np.mean(abs(outputs[i]-targets[i])/(targets[i]+outputs[i])/2)/len(outputs)
    print("sMAPE: {}%".format(sMAPE*100))
    return outputs, targets, sMAPE

lr = 0.001
gru_model = train(train_loader, lr, model_type="GRU")

loss_gru2=np.array([0.00020401 ,0.00016226])
loss_gru3=np.array([0.00015205 ,0.00012564])

Lstm_model = train(train_loader, lr, model_type="LSTM")

loss_lstm2=np.array([0.00024343,0.00012549])
loss_lstm3=np.array([0.00023406 ,0.00017392])

plt.plot(range(1,3),loss_rnn2,label="RNN 2 layer")
plt.plot(range(1,3),loss_rnn3,label="RNN 3 layer")
plt.plot(range(1,3),loss_lstm2,label="LSTM 2 layer")
plt.plot(range(1,3),loss_lstm3,label="LSTM 3 layer")
plt.plot(range(1,3),loss_gru2,label="GRU 2 layer")
plt.plot(range(1,3),loss_gru3, label="GRU 3 layer")
plt.legend()

gru_outputs, targets, gru_sMAPE = evaluate(gru_model, test_x, test_y, label_scalers,model_type="GRU")
lstm_outputs, targets, lstm_sMAPE = evaluate(Lstm_model, test_x, test_y, label_scalers,model_type="LSTM")
rnn_outputs, targets, rnn_sMAPE = evaluate(RNN_model, test_x, test_y, label_scalers,model_type="RNN")

plt.figure(figsize=(14,10))
plt.subplot(3,1,1)
plt.plot(gru_outputs[0][-100:], "-o", color="g", label="Predicted")
plt.plot(targets[0][-100:], color="b", label="Actual")
plt.ylabel('Energy Consumption (MW)')
plt.legend()
plt.title("GRU")

plt.subplot(3,1,2)
plt.plot(lstm_outputs[6][:100], "-o", color="g", label="Predicted")
plt.plot(targets[6][:100], color="b", label="Actual")
plt.ylabel('Energy Consumption (MW)')
plt.legend()
plt.title("LSTM")

plt.subplot(3,1,3)
plt.plot(rnn_outputs[6][:100], "-o", color="g", label="Predicted")
plt.plot(targets[6][:100], color="b", label="Actual")
plt.ylabel('Energy Consumption (MW)')
plt.legend()
plt.title("RNN")
plt.show()